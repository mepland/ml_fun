{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted From:  \n",
    "[Building Autoencoder in Pytorch - Vipul Vaibhaw](https://medium.com/@vaibhaw.vipul/building-autoencoder-in-pytorch-34052d1d280c)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "get_ipython().system('{sys.executable} -m pip install --upgrade pip');\n",
    "get_ipython().system('{sys.executable} -m pip install -r ../requirements.txt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    # img = img / 2 + 0.5 # unnormalize TODO\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and std deviations per channel for later normalization\n",
    "# do in minibatches, then take the mean over all the minibatches\n",
    "# adapted from: https://forums.fast.ai/t/image-normalization-in-pytorch/7534/7\n",
    "dataloader_unnormalized = torch.utils.data.DataLoader(\n",
    "    tv.datasets.CIFAR10(root='../data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=4096, shuffle=False, num_workers=8)\n",
    "\n",
    "pop_mean = []\n",
    "pop_std0 = []\n",
    "# pop_std1 = []\n",
    "for (images, labels) in tqdm(dataloader_unnormalized, desc='Minibatch'):\n",
    "    # shape = (minibatch_size, 3, 32, 32)\n",
    "    numpy_images = images.numpy()\n",
    "\n",
    "    # shape = (3,)\n",
    "    batch_mean = np.mean(numpy_images, axis=(0,2,3))\n",
    "    batch_std0 = np.std(numpy_images, axis=(0,2,3))\n",
    "    # batch_std1 = np.std(numpy_images, axis=(0,2,3), ddof=1)\n",
    "\n",
    "    pop_mean.append(batch_mean)\n",
    "    pop_std0.append(batch_std0)\n",
    "    # pop_std1.append(batch_std1)\n",
    "\n",
    "# shape = (num_minibatches, 3) -> (mean across 0th axis) -> shape (3,)\n",
    "pop_mean = np.array(pop_mean).mean(axis=0)\n",
    "pop_std0 = np.array(pop_std0).mean(axis=0)\n",
    "# pop_std1 = np.array(pop_std1).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop_mean\n",
    "# pop_mean = np.array([0.4916779 , 0.4823491 , 0.44675845], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop_std0\n",
    "# pop_std0 = np.array([0.24713232, 0.24348037, 0.26160604], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and transform the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),  transforms.Normalize(pop_mean, pop_std0)])\n",
    "\n",
    "trainset = tv.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "dataloader_train = torch.utils.data.DataLoader(trainset, batch_size=4096, shuffle=False, num_workers=8)\n",
    "\n",
    "testset = tv.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "dataloader_test = torch.utils.data.DataLoader(testset, batch_size=4096, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataiter = iter(dataloader_unnormalized)\n",
    "images, labels = dataiter.next()\n",
    "imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder,self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        self.decoder = nn.Sequential(             \n",
    "            nn.ConvTranspose2d(16, 6, kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(6, 3, kernel_size=5),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gpu support is available\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "print(f'cuda_avail = {cuda_avail}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "if cuda_avail:\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('WARNING Running on CPU!')\n",
    "    model.cpu()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a learning rate adjustment function that divides the learning rate by 10 every epoch_period=30 epochs, up to n_period_cap=6 times\n",
    "def adjust_learning_rate(epoch, initial_lr=0.001, epoch_period=30, n_period_cap=6):\n",
    "    exponent = min(n_period_cap, int(np.floor(epoch / epoch_period)))\n",
    "    lr = initial_lr / pow(10, exponent)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "def save_models(epoch):\n",
    "    torch.save(model.state_dict(), f'models/autoencoder_cifar10model_{epoch}.model')\n",
    "    print('Checkpoint saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for (images, labels) in dataloader_test:\n",
    "        if cuda_avail:\n",
    "            images = Variable(images.cuda())\n",
    "        else:\n",
    "            images = Variable(images.cpu())\n",
    "\n",
    "        # apply model and compute loss using images from the test set\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, images)\n",
    "        test_loss += loss.cpu().data.item() * images.size(0)\n",
    "\n",
    "    # Compute the average loss over all test images\n",
    "    test_loss = test_loss / len(dataloader_test.dataset)\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    best_loss = None\n",
    "\n",
    "    # for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        # for (images, labels) in tqdm(dataloader_train, desc='Minibatch'):\n",
    "        for (images, labels) in dataloader_train:\n",
    "\n",
    "            # Move images and labels to gpu if available\n",
    "            if cuda_avail:\n",
    "                images = Variable(images.cuda())\n",
    "            else:\n",
    "                images = Variable(images.cpu())\n",
    "\n",
    "            # Clear all accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, images)\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Adjust parameters according to the computed gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute loss\n",
    "            train_loss += loss.cpu().data.item() * images.size(0)\n",
    "\n",
    "        # Call the learning rate adjustment function\n",
    "        adjust_learning_rate(epoch)\n",
    "\n",
    "        # Compute the average acc and loss over all training images\n",
    "        train_loss = train_loss / len(dataloader_train.dataset)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        test_loss = test()\n",
    "\n",
    "        # Save the model if the test loss is less than our current best\n",
    "        if epoch == 0 or test_loss < best_loss:\n",
    "            save_models(epoch)\n",
    "            best_loss = test_loss\n",
    "\n",
    "        # Print the metrics\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl10 = tv.datasets.STL10(root='../data', split='train', folds=None, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl10[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(stl10[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tv.datasets.CIFAR10(root='../data', train=True, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(cifar10[6][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet = tv.datasets.ImageNet(root='TODO', split='train', download=True) # Can't download - need to torrent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
